# Portafolio de Pr√°cticas: Aprendizaje Computacional

Este repositorio re√∫ne una colecci√≥n de 5 proyectos pr√°cticos dise√±ados para explorar y validar experimentalmente algoritmos fundamentales de Machine Learning. 

Los proyectos abordan desaf√≠os t√©cnicos espec√≠ficos, desde la aproximaci√≥n de funciones matem√°ticas altamente no lineales y el an√°lisis de estructuras fractales (Mandelbrot), hasta la compresi√≥n de im√°genes mediante aprendizaje no supervisado.

## üõ†Ô∏è Stack Tecnol√≥gico
* **Lenguaje:** Python 3.x
* **Librer√≠as Core:** NumPy.
* **Machine Learning:** Scikit-Learn (Regresi√≥n, Clasificaci√≥n, Clustering, Model Selection).
* **Visualizaci√≥n:** Matplotlib.

## üìÇ Descripci√≥n de los Proyectos

### 1. Ingenier√≠a de Caracter√≠sticas en Regresi√≥n Lineal
* **Archivo:** `01_Linear_Regression_Sin_Approx.ipynb`
* **Reto:** Aproximar la funci√≥n no lineal $y = \sin(1/(x_1 \cdot x_2))$ en el intervalo $[-1,1]$ utilizando un modelo estrictamente lineal.
* **Metodolog√≠a:**
    * An√°lisis de curvas de aprendizaje (MSE/MAE/R¬≤) para diagnosticar *Underfitting*.
    * Visualizaci√≥n del error de estimaci√≥n mediante mapas de calor (*Heatmaps*).
    * **Mejora del modelo:** Implementaci√≥n de estrategias para capturar la no linealidad utilizando √∫nicamente `LinearRegression`.

### 2. Clasificaci√≥n de Fronteras Complejas con Regresi√≥n Log√≠stica
* **Archivo:** `02_Logistic_Regression_Sign_Classification.ipynb`
* **Reto:** Predecir el **signo** (positivo/negativo) de la funci√≥n oscilante $y = \sin(1/(x_1 \cdot x_2))$.
* **Modelo:** Regresi√≥n Log√≠stica.
* **Desaf√≠o T√©cnico:**
    * Evaluar el comportamiento de un clasificador lineal frente a una frontera de decisi√≥n geom√©trica compleja (patrones de ondas conc√©ntricas).
    * An√°lisis de las probabilidades predichas y la precisi√≥n del modelo en regiones de alta frecuencia.

### 3. Regresi√≥n en Fractales: SVR vs. MLP
* **Archivo:** `03_Mandelbrot_Regression_SVR_MLP.ipynb`
* **Objetivo:** Predicci√≥n de valores continuos derivados del conjunto de Mandelbrot (ej. iteraciones de escape).
* **Modelos:** * Support Vector Regression (SVR).
    * Perceptr√≥n Multicapa (MLP - Red Neuronal Artificial).
* **Metodolog√≠a:**
    * **Optimizaci√≥n de Hiperpar√°metros:** Uso de `GridSearchCV` para ajustar *kernels*, regularizaci√≥n ($C$) y par√°metros de la red neuronal.
    * Comparaci√≥n de la capacidad de generalizaci√≥n entre modelos basados en kernels y redes neuronales.

### 4. Clasificaci√≥n No Lineal: SVC vs. Naive Bayes
* **Archivo:** `04_Mandelbrot_Classification_SVC_NB.ipynb`
* **Objetivo:** Clasificaci√≥n binaria para determinar la pertenencia de un punto al conjunto de Mandelbrot.
* **Comparativa:**
    * **SVC (Support Vector Classifier):** Enfoque discriminativo maximizando el margen.
    * **Naive Bayes:** Enfoque probabil√≠stico generativo.
* **Implementaci√≥n:** Ajuste fino de modelos mediante b√∫squeda en rejilla (`GridSearchCV`) y an√°lisis de m√©tricas de clasificaci√≥n.

### 5. Cuantizaci√≥n de Im√°genes con K-Means (No Supervisado)
* **Archivo:** `05_Image_Quantization_KMeans.ipynb`
* **Aplicaci√≥n:** Compresi√≥n de im√°genes mediante la reducci√≥n de la paleta de colores (agrupamiento de p√≠xeles RGB).
* **An√°lisis Estad√≠stico:**
    * **Validaci√≥n Cruzada:** 10-fold Cross-Validation para asegurar la robustez del error (MSE/MAE).
    * **Benchmarking:** Uso de diagramas de caja (*Boxplots*) para comparar la estabilidad de K-Means frente a una asignaci√≥n de color aleatoria.
    * Evaluaci√≥n visual de la reconstrucci√≥n de la imagen variando el n√∫mero de clusters ($k=2, 5, 10...$).

---
**Nota:** Cada notebook contiene el flujo de trabajo completo: generaci√≥n/carga de datos, entrenamiento, validaci√≥n y discusi√≥n detallada de los resultados.
